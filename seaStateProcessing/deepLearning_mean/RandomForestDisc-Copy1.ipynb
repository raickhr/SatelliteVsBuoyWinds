{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca6412d-baef-41a6-b480-65e08ee88237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 16:20:29.529641: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-18 16:20:29.542588: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752870029.555863 1980831 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752870029.559874 1980831 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752870029.570834 1980831 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752870029.570846 1980831 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752870029.570847 1980831 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752870029.570848 1980831 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-18 16:20:29.574549: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Core scientific stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset, num2date, date2num\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, colors, cbook\n",
    "import seaborn as sns\n",
    "import cmocean as cmocn  # if you want it; comment out if not used\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.cluster import DBSCAN, OPTICS, KMeans, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import normalize, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (classification_report,\n",
    "                             accuracy_score,\n",
    "                             silhouette_score,\n",
    "                             pairwise_distances)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Scipy\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.spatial.distance import cdist, cosine\n",
    "from scipy.ndimage import convolve1d\n",
    "\n",
    "# Parallelism\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95353f4-ffbd-4676-ae03-1ec09f127949",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/srv/data2/srai_poseidon/srai_poseidon/observation/SatelliteVsBuoy/codes/seaStateProcessing/testMatchups/rainFlagRemovedBuoyDataBadQualityRemovedMatchupAmbuguitiesAdded_waveAndGlorysAdded_manualRemovedSomeData.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tfXarr/lib/python3.10/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tfXarr/lib/python3.10/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/srv/data2/srai_poseidon/srai_poseidon/observation/SatelliteVsBuoy/codes/seaStateProcessing/testMatchups/rainFlagRemovedBuoyDataBadQualityRemovedMatchupAmbuguitiesAdded_waveAndGlorysAdded_manualRemovedSomeData.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '70543eb3-7347-4d82-b665-5039f2141aa8']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../testMatchups/rainFlagRemovedBuoyDataBadQualityRemovedMatchupAmbuguitiesAdded_waveAndGlorysAdded_manualRemovedSomeData.nc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#ds['mean_WVEL'] = np.sqrt(ds['mean_U10N_x'] **2 + ds['mean_U10N_y']**2)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mto_dataframe()\n",
      "File \u001b[0;32m~/anaconda3/envs/tfXarr/lib/python3.10/site-packages/xarray/backends/api.py:687\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    676\u001b[0m     decode_cf,\n\u001b[1;32m    677\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    683\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    684\u001b[0m )\n\u001b[1;32m    686\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 687\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    694\u001b[0m     backend_ds,\n\u001b[1;32m    695\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    706\u001b[0m )\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/anaconda3/envs/tfXarr/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:666\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    646\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m ReadBuffer \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    663\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    664\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    665\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 666\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/anaconda3/envs/tfXarr/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:452\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, auto_complex, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    448\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_complex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m auto_complex\n\u001b[1;32m    449\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    450\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    451\u001b[0m )\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfXarr/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:393\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/anaconda3/envs/tfXarr/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:461\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfXarr/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:455\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    456\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/anaconda3/envs/tfXarr/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tfXarr/lib/python3.10/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/anaconda3/envs/tfXarr/lib/python3.10/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2521\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2158\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/srv/data2/srai_poseidon/srai_poseidon/observation/SatelliteVsBuoy/codes/seaStateProcessing/testMatchups/rainFlagRemovedBuoyDataBadQualityRemovedMatchupAmbuguitiesAdded_waveAndGlorysAdded_manualRemovedSomeData.nc'"
     ]
    }
   ],
   "source": [
    "fname = '../testMatchups/rainFlagRemovedBuoyDataBadQualityRemovedMatchupAmbuguitiesAdded_waveAndGlorysAdded_manualRemovedSomeData.nc'\n",
    "ds = xr.open_dataset(fname)\n",
    "#ds['mean_WVEL'] = np.sqrt(ds['mean_U10N_x'] **2 + ds['mean_U10N_y']**2)\n",
    "data = ds.to_dataframe()\n",
    "orig_df = data.copy()\n",
    "df = data.copy()\n",
    "\n",
    "df['SST-AIRT'] = df['Sea Surface Temperature (TAO)'] - df['Air Temperature (TAO)']\n",
    "\n",
    "# C = df['mean_cosWDIR']\n",
    "# S = df['mean_sinWDIR']\n",
    "# R = np.sqrt(C**2 + S**2)\n",
    "# sigma = np.rad2deg(np.sqrt(-np.log(R)))\n",
    "# df['std_WDIR'] = sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf69b4dd-a9d8-4a78-a102-755abb75582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5601411-77a6-4091-ba07-e2c5d7430833",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ad6fd-c322-4332-9345-c482b900d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot( df['Wind Direction (QuikSCAT)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e679d-34ce-42d9-ae1e-384c90c14a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mean Speed Difference (QuikSCAT - TAO)'] = df['Wind Speed (QuikSCAT)'] - df['mean_WSPD_10N']\n",
    "mean_dir = (np.rad2deg(np.arctan2(df['mean_U10N_y'], df['mean_U10N_x']))+360)%360 \n",
    "dir_diff = df['Wind Direction (QuikSCAT)'] - mean_dir\n",
    "df['mean cos(Direction Difference (QuikSCAT - TAO))'] = np.cos(np.deg2rad(dir_diff))\n",
    "df['mean sin(Direction Difference (QuikSCAT - TAO))'] = np.sin(np.deg2rad(dir_diff))\n",
    "dir_diff[dir_diff > 180] = dir_diff[dir_diff > 180] -360\n",
    "dir_diff[dir_diff <- 180] = dir_diff[dir_diff < -180] + 360\n",
    "df['mean Direction Difference (QuikSCAT - TAO)'] = dir_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbabcf0-f081-40dd-bf0a-a033f69db888",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectX = ['mean Speed Difference (QuikSCAT - TAO)',\n",
    "          'mean cos(Direction Difference (QuikSCAT - TAO))',\n",
    "          'mean sin(Direction Difference (QuikSCAT - TAO))']\n",
    "\n",
    "\n",
    "X = df[selectX]\n",
    "\n",
    "normX = (X - X.mean(axis=0))/ X.std(axis=0)\n",
    "\n",
    "# kmeans = KMeans(n_clusters=9, random_state=0, n_init=\"auto\", max_iter = 10000).fit(normX)\n",
    "# df['label'] = kmeans.labels_\n",
    "\n",
    "#dbscan = DBSCAN(eps=0.15, min_samples= 500)\n",
    "dbscan = DBSCAN(eps=0.13, min_samples= 500)\n",
    "df['label'] = dbscan.fit_predict(normX)\n",
    "\n",
    "# optics = OPTICS(min_samples=50 ) #cluster_method='dbscan', eps=0.25)\n",
    "# df['label'] = optics.fit_predict(normX)\n",
    "\n",
    "fig,  axes = plt.subplots(ncols = 2, figsize=(16,6))\n",
    "\n",
    "ax = axes[0]\n",
    "# ax.remove()  # Remove the existing second subplot\n",
    "# ax = fig.add_subplot(121, projection='polar')\n",
    "xlabel = 'mean Speed Difference (QuikSCAT - TAO)'\n",
    "ylabel = 'mean Direction Difference (QuikSCAT - TAO)'\n",
    "\n",
    "s = sns.histplot(df, x=xlabel, y = ylabel,#levels=10, \n",
    "                hue='label', common_norm=True, cbar = True,\n",
    "                palette = sns.color_palette(\"bright\"), ax = ax)\n",
    "\n",
    "ax.grid(visible=True, which='both', axis='both')\n",
    "\n",
    "ax.text(0.1, 0.95, 'A', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = 20)\n",
    "\n",
    "ax.set_xlim(-10,15)\n",
    "\n",
    "ax = axes[1]\n",
    "s = sns.countplot(df, x='label',palette = sns.color_palette(\"bright\"), ax = ax)\n",
    "s.bar_label(s.containers[0])\n",
    "\n",
    "ax.grid(visible=True, which='both', axis='both')\n",
    "ax.set_xlabel('cluster label')\n",
    "#ax.set_ylabel('cluster label')\n",
    "\n",
    "ax.text(0.1, 0.95, 'B', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = 20)\n",
    "\n",
    "\n",
    "#plt.savefig('PDF_byCluster_10min.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c097609-d4ea-47e8-be4f-ccc263c1f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "goodBad = np.array(['good' for i in range(len(df))])\n",
    "print(len(goodBad))\n",
    "mask = df['label'].isin([-1])\n",
    "goodBad[mask] = 'bad'\n",
    "df['GoodBad'] = goodBad\n",
    "\n",
    "def getGoodBad(lat, lon, df):\n",
    "    isPresent = False\n",
    "    lon = (lon + 360)%360\n",
    "    subDF = df.loc[df['LATITUDE'] == lat]\n",
    "    subDF = subDF.loc[subDF['LONGITUDE'] == lon]\n",
    "    if len(subDF) > 0:\n",
    "        isPresent = True\n",
    "    if isPresent:    \n",
    "        goodCount = len(subDF.loc[subDF['GoodBad'] == 'good'])/len(subDF)*100\n",
    "        badCount = len(subDF.loc[subDF['GoodBad'] == 'bad'])/len(subDF)*100\n",
    "        return f' {goodCount:3.0f},{badCount:3.0f}'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def getBadPercent(lat, lon, df):\n",
    "    isPresent = False\n",
    "    lon = (lon + 360)%360\n",
    "    subDF = df.loc[df['LATITUDE'] == lat]\n",
    "    subDF = subDF.loc[subDF['LONGITUDE'] == lon]\n",
    "    if len(subDF) > 0:\n",
    "        isPresent = True\n",
    "    \n",
    "    if isPresent:\n",
    "        goodCount = len(subDF.loc[subDF['GoodBad'] == 'good'])/len(subDF)*100\n",
    "        badCount = len(subDF.loc[subDF['GoodBad'] == 'bad'])/len(subDF)*100\n",
    "        \n",
    "        return badCount\n",
    "    else:\n",
    "        return -999\n",
    "\n",
    "def getCount(lat, lon, df):\n",
    "    isPresent = False\n",
    "    lon = (lon + 360)%360\n",
    "    subDF = df.loc[df['LATITUDE'] == lat]\n",
    "    subDF = subDF.loc[subDF['LONGITUDE'] == lon]\n",
    "    if len(subDF) > 0:\n",
    "        isPresent = True\n",
    "    if isPresent:\n",
    "        count = len(subDF) #.loc[subDF['GoodBad'] == 'good'])/len(subDF)*100\n",
    "        return count\n",
    "    else:\n",
    "        return -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7307f510-7ffd-49c8-8bdb-b3a96225daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame: df must contain 'label', 'speed', 'direction'\n",
    "# Replace these with your actual column names\n",
    "xlabel = 'mean Speed Difference (QuikSCAT - TAO)'\n",
    "ylabel = 'mean Direction Difference (QuikSCAT - TAO)'\n",
    "\n",
    "# Compute mean and std\n",
    "mu_speed = df[xlabel].mean()\n",
    "std_speed = df[xlabel].std()\n",
    "mu_dir = df[ylabel].mean()\n",
    "std_dir = df[ylabel].std()\n",
    "\n",
    "# Create overall figure\n",
    "fig = plt.figure(figsize=(13, 10))\n",
    "gs = gridspec.GridSpec(4, 8, width_ratios=[3, 0.8, 0.1, 0.1, 0.25, 0.1, 0.75, 1], height_ratios=[0.75, 3, 0.75, 2], wspace=0, hspace=0)\n",
    "\n",
    "# Joint and marginals (LEFT)\n",
    "ax_joint = fig.add_subplot(gs[1, 0])\n",
    "ax_marg_x = fig.add_subplot(gs[0, 0], sharex=ax_joint)\n",
    "ax_marg_y = fig.add_subplot(gs[1, 1], sharey=ax_joint)\n",
    "ax_cb1 = fig.add_subplot(gs[1,3])\n",
    "ax_cb2 = fig.add_subplot(gs[1,5])\n",
    "\n",
    "# Countplot (RIGHT)\n",
    "ax_count = fig.add_subplot(gs[0:2, -1])  # spans both rows\n",
    "\n",
    "# -------------------------\n",
    "# 1. JOINT HISTOGRAM + KDE\n",
    "\n",
    "colors = [plt.cm.Blues, plt.cm.Oranges]\n",
    "hexcounts = [100, 100]\n",
    "x_min, x_max, y_min, y_max = -5, 5, -100, 100\n",
    "vmins = [0, 0]\n",
    "vmaxs = [100, 500]\n",
    "numcontours = 5\n",
    "\n",
    "for i, (label, ax, ax_cb) in enumerate(zip([-1, 0], [ax_joint, ax_joint], [ax_cb1, ax_cb2])):\n",
    "    df_label = df[df['label'] == label]\n",
    "\n",
    "    hb = ax.hexbin(\n",
    "        x=df_label[xlabel],\n",
    "        y=df_label[ylabel],\n",
    "        gridsize=hexcounts[i],\n",
    "        cmap=colors[i],\n",
    "        mincnt=1,\n",
    "        linewidths=0.01,\n",
    "        edgecolors=(0, 0, 0, 0.01),\n",
    "        vmin=vmins[i],            # â† custom lower limit\n",
    "        vmax=vmaxs[i],             # â† custom upper limit\n",
    "        extent=(x_min, x_max, y_min, y_max)\n",
    "    )\n",
    "\n",
    "    # Extract colorbar for this label\n",
    "    cbar = fig.colorbar(hb, cax=ax_cb)\n",
    "    \n",
    "#cbar.set_label(\"Bin Count\")\n",
    "cbar.ax.text(\n",
    "    0.6, 1.05,                        # x=50% of width, y=just above the top\n",
    "    \"Bin Count\",                     # your label\n",
    "    ha='right', va='bottom',        # horizontal center, baseline aligned\n",
    "    fontsize=10, fontweight='bold',  # optional styling\n",
    "    transform=cbar.ax.transAxes      # relative to colorbar axis\n",
    ")\n",
    "\n",
    "# Create the Joint PDF plot and capture the returned QuadContourSet object\n",
    "# KDE Grid resolution\n",
    "nx, ny = 64, 64\n",
    "\n",
    "# Data\n",
    "x = df[xlabel].values\n",
    "y = df[ylabel].values\n",
    "xy = np.vstack([x, y])\n",
    "\n",
    "# KDE with specified bandwidth\n",
    "kde = gaussian_kde(xy, bw_method=0.1)\n",
    "\n",
    "# Grid over which to evaluate\n",
    "xx, yy = np.mgrid[x_min:x_max:nx*1j, y_min:y_max:ny*1j]\n",
    "grid_coords = np.vstack([xx.ravel(), yy.ravel()])\n",
    "z = kde(grid_coords).reshape(xx.shape)\n",
    "\n",
    "# Normalize z to make it a probability surface (area â‰ˆ 1)\n",
    "dx = (x_max - x_min) / nx\n",
    "dy = (y_max - y_min) / ny\n",
    "z_prob = z / z.sum() #/ (dx * dy)\n",
    "\n",
    "# Flatten and sort to get cumulative distribution\n",
    "z_flat = z_prob.flatten()\n",
    "z_sorted = np.sort(z_flat)[::-1]  # ðŸ”„ descending order\n",
    "z_cumsum = np.cumsum(z_sorted)\n",
    "\n",
    "# Desired probability masses for contours\n",
    "mass_levels = [0.1, 0.5, 0.68, 0.8, 0.95]\n",
    "mass_levels.reverse()\n",
    "levels = [z_sorted[np.searchsorted(z_cumsum, mass)] for mass in mass_levels]\n",
    "\n",
    "\n",
    "#levels.sort()\n",
    "# Plot contours\n",
    "contour = ax_joint.contour(xx, yy, z_prob, levels=levels, colors='black', linewidths=0.8)\n",
    "\n",
    "# Label with probability instead of density\n",
    "fmt = {}\n",
    "for l, s in zip(contour.levels, mass_levels):\n",
    "    fmt[l] = f\"{s:.3f}\"  # show enclosed probability\n",
    "\n",
    "#fmt = {level: f\"{mass:.3f}\" for level, mass in zip(levels, mass_levels)}\n",
    "\n",
    "ax_joint.clabel(contour, contour.levels, inline=True, fmt=fmt, fontsize=8)\n",
    "# # Add labels to the contour lines\n",
    "# ax_joint.clabel(contour, inline=True, fontsize=8, fmt=\"%.2f\")\n",
    "\n",
    "# 2. MARGINAL KDEs\n",
    "sns.kdeplot(data=df, x=xlabel, fill=True, legend=False, ax=ax_marg_x, clip = (x_min, x_max) , cut = 0, bw_adjust=0.5 )\n",
    "sns.kdeplot(data=df, y=ylabel, fill=True, legend=False, ax=ax_marg_y, clip = (y_min, y_max) , cut = 0, bw_adjust=0.5 )\n",
    "\n",
    "# Remove marginal axis ticks\n",
    "ax_marg_x.tick_params(bottom=False, labelbottom=False)\n",
    "ax_marg_y.tick_params(left=False, labelleft=False)\n",
    "\n",
    "# Add Â±1Ïƒ lines\n",
    "for x in [mu_speed - std_speed, mu_speed, mu_speed + std_speed]:\n",
    "    ax_joint.axvline(x, color='red', linestyle='--', linewidth=1)\n",
    "    ax_marg_x.axvline(x, color='red', linestyle='--', linewidth=1)  \n",
    "    ax_marg_x.set_yticks([0.2,0.4])\n",
    "\n",
    "for y in [mu_dir - std_dir, mu_dir, mu_dir + std_dir]:\n",
    "    ax_joint.axhline(y, color='green', linestyle='--', linewidth=1)\n",
    "    ax_marg_y.axhline(y, color='green', linestyle='--', linewidth=1)\n",
    "    ax_marg_y.set_xticks([0.02,0.04])\n",
    "\n",
    "\n",
    "# Axis labels\n",
    "ax_joint.set_xlabel(xlabel + ' [m/s]')\n",
    "ax_joint.set_ylabel(ylabel + \" (Â°)\")\n",
    "ax_marg_x.tick_params(axis='x', which='both', bottom=False, labelbottom=False)\n",
    "ax_marg_y.tick_params(axis='y', which='both', left=False, labelleft=False)\n",
    "ax_marg_x.set_xlabel(None)\n",
    "ax_marg_y.set_ylabel(None)\n",
    "\n",
    "# Label the panel\n",
    "ax_joint.text(0.05, 0.95, 'A. (i)', transform=ax_joint.transAxes, fontsize=16, weight='bold')\n",
    "ax_marg_x.text(0.05, 0.80, 'A. (ii)', transform=ax_marg_x.transAxes, fontsize=16, weight='bold')\n",
    "ax_marg_y.text(0.05, 0.95, 'A. (iii)', transform=ax_marg_y.transAxes, fontsize=16, weight='bold')\n",
    "\n",
    "# -------------------------\n",
    "# 3. COUNTPLOT\n",
    "s = sns.countplot(data=df, x='label', palette='bright', ax=ax_count)\n",
    "for container in s.containers:\n",
    "    s.bar_label(container)\n",
    "ax_count.set_title(\"Cluster Label Counts\")\n",
    "#ax_count.set_xticks([-1, 0])                    # positions (the actual cluster labels)\n",
    "ax_count.set_xticklabels([\"Bad\", \"Good\"])       # the new labels shown on the axis\n",
    "ax_count.set_xlabel(\"Matchup Quality\")\n",
    "ax_count.set_ylabel(\"Count\")\n",
    "ax_count.text(0.05, 0.95, 'B.', transform=ax_count.transAxes, fontsize=16, weight='bold')\n",
    "\n",
    "# Final adjustments\n",
    "\n",
    "ax_joint.set_xlim(x_min, x_max)\n",
    "ax_joint.set_ylim(y_min,  y_max)\n",
    "ax_joint.grid(lw = 0.2)\n",
    "\n",
    "ax_marg_x.set_xlim(x_min, x_max)\n",
    "ax_marg_x.grid(lw = 0.2)\n",
    "\n",
    "ax_marg_y.set_ylim(y_min,  y_max)\n",
    "ax_marg_y.grid(lw = 0.2)\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature \n",
    "dotSize = 60\n",
    "slw = 0.1\n",
    "\n",
    "latList = [-9, -8, -5, -2, 0, 2, 5, 8, 9]\n",
    "lonList = [-95, -110, -125, -140, -155, -170, -180, 165]\n",
    "\n",
    "ylen = len(latList)\n",
    "xlen = len(lonList)\n",
    "\n",
    "taskList = []\n",
    "\n",
    "for latId  in range(ylen):\n",
    "    for lonId in range(xlen):\n",
    "        taskList.append([latList[latId], lonList[lonId]])\n",
    "\n",
    "ntasks = len(taskList)\n",
    "\n",
    "ax = fig.add_subplot(gs[3,:], projection=ccrs.PlateCarree(central_longitude=180))\n",
    "ax.set_aspect(1.2)\n",
    "\n",
    "land = cfeature.NaturalEarthFeature(\n",
    "    category='physical',\n",
    "    name='land',\n",
    "    scale='50m',\n",
    "    facecolor='lightgrey'  # Set the color to grey\n",
    ")\n",
    "\n",
    "ax.add_feature(land)\n",
    "\n",
    "\n",
    "ax.set_extent([140, -70, -5, 5]) \n",
    "gridlines = ax.gridlines(draw_labels=True)\n",
    "ax.coastlines()\n",
    "plotList = np.zeros((0,3), dtype=float)\n",
    "for task in taskList:\n",
    "    lat = task[0]\n",
    "    lon = task[1]\n",
    "    txt = getGoodBad(lat, lon, df)\n",
    "    \n",
    "\n",
    "    xpos = lon + 180\n",
    "    \n",
    "    if xpos > 180:\n",
    "       xpos -= 360\n",
    "    if txt != '': \n",
    "        badPercent = getBadPercent(lat, lon, df)\n",
    "        #print(badPercent)\n",
    "        txt = f'{badPercent:2.0f}'\n",
    "        ax.text(xpos+2, lat-0.5, txt)\n",
    "        plotList = np.concatenate((plotList, np.array([[xpos, lat, badPercent]])), axis = 0)\n",
    "\n",
    "x = ax.scatter(plotList[:,0], plotList[:,1], c=plotList[:,2], s=dotSize, edgecolor='black', linewidths=slw, cmap=cm.turbo)    \n",
    "cb = plt.colorbar(x, ax = ax)\n",
    "cb.ax.set_title('% of Bad Matchups')\n",
    "#ax.scatter(df['LONGITUDE'], df['LATITUDE'], transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.text(0.1, 0.95, 'C. ', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = 15)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('clustering.pdf', dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1988210e-0877-4c53-a50e-97d8a6384d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel = 'mean Speed Difference (QuikSCAT - TAO)'\n",
    "ylabel = 'mean Direction Difference (QuikSCAT - TAO)'\n",
    "\n",
    "subDF = df.copy()\n",
    "\n",
    "subDF['norm_xlabel'] = 2 * (subDF[xlabel] - subDF[xlabel].min()) / (subDF[xlabel].max() - subDF[xlabel].min()) - 1\n",
    "subDF['norm_ylabel'] = 2 * (subDF[ylabel] - subDF[ylabel].min()) / (subDF[ylabel].max() - subDF[ylabel].min()) - 1\n",
    "\n",
    "subDF['norm_xlabel'] = (subDF['norm_xlabel'] - subDF['norm_xlabel'].mean() + 0.01) #/ (subDF['norm_xlabel'].std())\n",
    "subDF['norm_ylabel'] = (subDF['norm_ylabel'] - subDF['norm_ylabel'].mean()) #/ (subDF['norm_ylabel'].std())\n",
    "\n",
    "# subDF['norm_xlabel'] = (subDF[xlabel] - subDF[xlabel].mean()) / (subDF[xlabel].std())\n",
    "# subDF['norm_ylabel'] = (subDF[ylabel] - subDF[ylabel].mean()) / (subDF[ylabel].std())\n",
    "\n",
    "# subDF['norm_xlabel'] = 2 * (subDF['norm_xlabel'] - subDF['norm_xlabel'].min()) / (subDF['norm_xlabel'].max() - subDF['norm_xlabel'].min()) - 1\n",
    "# subDF['norm_ylabel'] = 2 * (subDF['norm_ylabel'] - subDF['norm_ylabel'].min()) / (subDF['norm_ylabel'].max() - subDF['norm_ylabel'].min()) - 1\n",
    "\n",
    "subDF['theta'] = np.rad2deg(np.arctan2(subDF['norm_ylabel'], subDF['norm_xlabel']))\n",
    "subDF['r'] = np.sqrt(subDF['norm_ylabel']**2 + subDF['norm_xlabel']**2)\n",
    "\n",
    "subDF['org_theta'] = np.rad2deg(np.arctan2(subDF[ylabel], subDF[xlabel]))\n",
    "subDF['org_r'] = np.sqrt(subDF[ylabel]**2 + subDF[xlabel]**2)\n",
    "\n",
    "# df['theta'] = np.rad2deg(np.arctan2(df[ylabel], df[xlabel]))\n",
    "# df['r'] = np.sqrt(df[ylabel]**2 + df[xlabel]**2)\n",
    "\n",
    "# Define bin edges using NumPy (0 to 360 in 0.25Â° steps)\n",
    "num_bins = 360*6\n",
    "bin_edges = np.linspace(0, 360, num_bins, endpoint = False)  # 1441 edges for 1440 bins\n",
    "right_edges = np.roll(bin_edges, -1)\n",
    "right_edges[-1] = 360\n",
    "bin_centers = (bin_edges + right_edges)/2\n",
    "# Digitize theta into bins\n",
    "subDF['org_theta_bin'] = np.digitize((subDF['org_theta'] + 360)% 360, bins=bin_edges, right=False)\n",
    "subDF['org_theta_bin'] = subDF['org_theta_bin'] % num_bins  # Ensure cyclic wrap-around\n",
    "\n",
    "\n",
    "subDF['theta_bin'] = np.digitize((subDF['theta'] +360) % 360, bins=bin_edges, right=False)\n",
    "subDF['theta_bin'] = subDF['theta_bin'] % num_bins  # Ensure cyclic wrap-around\n",
    "\n",
    "ssubDF = subDF.loc[df['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9bd216-1932-4695-bcb3-83e43fdbd215",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968faba6-df82-491b-82c3-5ef737afd966",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(subDF['org_theta_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4389e-8c14-4887-911b-009ea1d02a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxr = []\n",
    "minr = []\n",
    "for i in range(num_bins):\n",
    "    thisDF = ssubDF.loc[ssubDF['org_theta_bin'] == i]\n",
    "    maxr.append(np.max(thisDF['org_r']))\n",
    "    minr.append(np.min(thisDF['org_r']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b36e45-558a-493c-b438-a368f525ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxr = np.array(maxr)\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8177808-c7df-4721-8839-ee43476e8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInsideEllipse(x, y, a, b, x1, y1):\n",
    "    LHS = ((x - x1)**2)/a**2 + ((y-y1)**2)/b**2\n",
    "    labels = np.array(LHS < 1, dtype=int)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f85cd5f-7758-4ea1-882c-1f87c2c0738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.6\n",
    "b = 20\n",
    "x1 = -0.075\n",
    "y1 = -1\n",
    "\n",
    "A = b**2 * np.cos(np.deg2rad(bin_centers))**2 +  a**2 * np.sin(np.deg2rad(bin_centers))**2\n",
    "B = -2 * (x1 * np.cos(np.deg2rad(bin_centers)) * b**2 + y1 * np.sin(np.deg2rad(bin_centers)) * a**2 )  \n",
    "C = x1**2* b**2 + y1**2 * a**2 - a**2 * b**2\n",
    "\n",
    "r1 = (-B + np.sqrt(B**2 - 4* A * C))/(2* A)\n",
    "#r2 = (-B - np.sqrt(B**2 - 4* A * C))/(2* A)\n",
    "\n",
    "r = np.sqrt(a **2 * b**2 /(a**2 * np.sin(np.deg2rad(bin_centers))**2 + b**2 * np.cos(np.deg2rad(bin_centers))**2))\n",
    "plt.scatter(maxr * np.cos(np.deg2rad(bin_centers)), maxr * np.sin(np.deg2rad(bin_centers)), s = 1)\n",
    "#plt.scatter(r2 * np.cos(np.deg2rad(bin_centers)), r2 * np.sin(np.deg2rad(bin_centers)), s = 1)\n",
    "plt.scatter(r1 * np.cos(np.deg2rad(bin_centers)), r1 * np.sin(np.deg2rad(bin_centers)), s = 1)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847ad57-7856-45f2-a0e6-32ededc6273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipseLabel = isInsideEllipse(df[xlabel].to_numpy(), df[ylabel].to_numpy(), a, b, x1, y1)\n",
    "subDF['ellipseLabel'] = ellipseLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5989fa-ec1a-47c9-924c-311a4458520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,  axes = plt.subplots(ncols = 2, figsize=(16,6))\n",
    "\n",
    "ax = axes[0]\n",
    "# ax.remove()  # Remove the existing second subplot\n",
    "# ax = fig.add_subplot(121, projection='polar')\n",
    "xlabel = 'Speed Difference (QuikSCAT - TAO)'\n",
    "ylabel = 'Direction Difference (QuikSCAT - TAO)'\n",
    "\n",
    "s = sns.histplot(df, x=xlabel, y = ylabel,#levels=10, \n",
    "                hue='label', common_norm=True, cbar = True,\n",
    "                palette = sns.color_palette(\"bright\"), ax = ax)\n",
    "\n",
    "ax.grid(visible=True, which='both', axis='both')\n",
    "\n",
    "ax.text(0.1, 0.95, 'A', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = 20)\n",
    "\n",
    "ax.set_xlim(-10,15)\n",
    "\n",
    "ax = axes[1]\n",
    "s = sns.countplot(df, x='label',palette = sns.color_palette(\"bright\"), ax = ax)\n",
    "s.bar_label(s.containers[0])\n",
    "\n",
    "ax.grid(visible=True, which='both', axis='both')\n",
    "ax.set_xlabel('cluster label')\n",
    "#ax.set_ylabel('cluster label')\n",
    "\n",
    "ax.text(0.1, 0.95, 'B', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69aa72-cd3c-4bc8-9429-977c39e71a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,  axes = plt.subplots(ncols = 2, figsize=(16,6))\n",
    "\n",
    "ax = axes[0]\n",
    "# ax.remove()  # Remove the existing second subplot\n",
    "# ax = fig.add_subplot(121, projection='polar')\n",
    "xlabel = 'mean Speed Difference (QuikSCAT - TAO)'\n",
    "ylabel = 'mean Direction Difference (QuikSCAT - TAO)'\n",
    "\n",
    "s = sns.histplot(subDF, x=xlabel, y = ylabel,#levels=10, \n",
    "                hue='ellipseLabel', common_norm=True, cbar = True,\n",
    "                palette = sns.color_palette(\"bright\"), ax = ax)\n",
    "\n",
    "ax.grid(visible=True, which='both', axis='both')\n",
    "\n",
    "ax.text(0.1, 0.95, 'A', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = 20)\n",
    "\n",
    "ax.set_xlim(-10,15)\n",
    "\n",
    "ax = axes[1]\n",
    "s = sns.countplot(subDF, x='ellipseLabel',palette = sns.color_palette(\"bright\"), ax = ax)\n",
    "s.bar_label(s.containers[0])\n",
    "\n",
    "ax.grid(visible=True, which='both', axis='both')\n",
    "ax.set_xlabel('cluster label')\n",
    "#ax.set_ylabel('cluster label')\n",
    "\n",
    "ax.text(0.1, 0.95, 'B', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331fdfe-d7a0-42a3-92f3-6c8cbad2387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(subDF['norm_xlabel']), np.max(subDF['norm_xlabel']))\n",
    "print(np.min(subDF['norm_ylabel']), np.max(subDF['norm_ylabel']))\n",
    "print(np.min(ssubDF['r']), np.max(ssubDF['r']))\n",
    "print(np.min(subDF['r']), np.max(subDF['r']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23c04b-c7cc-4e23-862b-841c840d71c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ssubDF['r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a956f326-f1cf-4b69-b981-427343dffdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = 0.09\n",
    "r2 = 0.13\n",
    "ssubDF = subDF.loc[np.logical_and(subDF['r'].to_numpy() > r1, subDF['r'].to_numpy() < r2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9bfa9b-b762-49f1-9b04-0c1b2d91e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,  axes = plt.subplots(ncols = 2, figsize=(16,6))\n",
    "\n",
    "ax = axes[0]\n",
    "# ax.remove()  # Remove the existing second subplot\n",
    "# ax = fig.add_subplot(121, projection='polar')\n",
    "xlabel = 'mean Speed Difference (QuikSCAT - TAO)'\n",
    "ylabel = 'mean Direction Difference (QuikSCAT - TAO)'\n",
    "\n",
    "s = sns.histplot(ssubDF, x=xlabel, y = ylabel, #levels=10, \n",
    "                hue='label', common_norm=True, cbar = True,\n",
    "                palette = sns.color_palette(\"bright\"), ax = ax)\n",
    "\n",
    "ax.grid(visible=True, which='both', axis='both')\n",
    "\n",
    "ax.text(0.1, 0.95, 'A', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = 20)\n",
    "\n",
    "ax.set_xlim(-5,5)\n",
    "\n",
    "ax = axes[1]\n",
    "s = sns.countplot(ssubDF, x='label',palette = sns.color_palette(\"bright\"), ax = ax)\n",
    "s.bar_label(s.containers[0])\n",
    "\n",
    "ax.grid(visible=True, which='both', axis='both')\n",
    "ax.set_xlabel('cluster label')\n",
    "#ax.set_ylabel('cluster label')\n",
    "\n",
    "ax.text(0.1, 0.95, 'B', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = 20)\n",
    "\n",
    "\n",
    "#plt.savefig('PDF_byCluster_10min.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137153e-fc41-4adf-967b-51df39182dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.logical_and(subDF['r'].to_numpy() > r1, subDF['r'].to_numpy() < r2)\n",
    "subDF = subDF.loc[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e96ca8-2b20-4134-a73d-81e901230b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(subDF.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e211779-aa8e-4110-b19f-22e5f34fa8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subDF = subDF.rename_vars({'nudge_wind_speed':'model_speed',\n",
    "#                            'nudge_wind_direction':'model_dir'})\n",
    "\n",
    "subDF = subDF.rename(columns={\n",
    "    'nudge_wind_speed': 'model_speed',\n",
    "    'nudge_wind_direction': 'model_dir'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94933398-c7be-458f-80ff-4e873e0b5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "subDF['label'] = subDF['label'].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "# Features to use for classification\n",
    "features = [#'std_WDIR',\n",
    "            'std_cosWDIR', \n",
    "            'std_sinWDIR',\n",
    "            'SST-AIRT',\n",
    "            'mean_SST - AIRT',\n",
    "            'std_SST - AIRT', \n",
    "            'Relative Humidity (TAO)',             \n",
    "            'mean_RELH',             \n",
    "            'mean_SST',\n",
    "            'std_AIRT',\n",
    "            'std_SST', \n",
    "            'mean_AIRT',\n",
    "            'std_RELH',\n",
    "            'Neutral Wind Speed at 10m (TAO)',\n",
    "            'std_WSPD', \n",
    "            'mean_WSPD',\n",
    "            'mean_WSPD_10N',\n",
    "            'std_WSPD_10N',\n",
    "            #'model_speed',\n",
    "            #'model_dir',\n",
    "            'VHM0',\n",
    "             'VHM0_SW1',\n",
    "             'VHM0_SW2',\n",
    "             'VHM0_WW',\n",
    "             'VMDR',\n",
    "             'VMDR_SW1',\n",
    "             'VMDR_SW2',\n",
    "             'VMDR_WW',\n",
    "             'VPED',\n",
    "             'VSDX',\n",
    "             'VSDY',\n",
    "             'VTM01_SW1',\n",
    "             'VTM01_SW2',\n",
    "             'VTM01_WW',\n",
    "             'VTM02',\n",
    "             'VTM10',\n",
    "             'VTPK',\n",
    "             'vo',\n",
    "             'zos',\n",
    "             'uo',\n",
    "             'thetao',\n",
    "             'so'        \n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62d471-0b6b-4ff3-ad90-b52280027af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target (y)\n",
    "X = subDF[features]\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than threshold (e.g., 0.9)\n",
    "threshold = 0.9\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "# Drop them\n",
    "df_reduced = X.drop(columns=to_drop)\n",
    "\n",
    "print(f\"Removed {len(to_drop)} features: {to_drop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa289d-7043-4b51-ad4d-1d09d5e70203",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 20))\n",
    "sns.heatmap(corr_matrix, \n",
    "            annot=True,      # show the correlation coefficients\n",
    "            fmt=\".2f\",        # format with 2 decimal places\n",
    "            cmap='coolwarm',  # color map\n",
    "            square=True, \n",
    "            linewidths=0.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c057832-b078-4e50-8674-7b76b7569f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df_reduced.columns)\n",
    "features, len(features)\n",
    "#print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58dbf2e-fe46-426f-b6b9-1365e6a0a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = subDF[features]\n",
    "y = subDF['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize and train a RandomForest Classifier\n",
    "classifier = RandomForestClassifier(n_estimators=500,\n",
    "                                    max_depth=5,\n",
    "                                    min_samples_leaf= 20,\n",
    "                                    min_samples_split= 10,\n",
    "                                    max_features=0.5,  \n",
    "                                    bootstrap=True,\n",
    "                                    random_state=42,\n",
    "                                    class_weight='balanced',\n",
    "                                    n_jobs=-1)\n",
    "classifier.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier performance\n",
    "print(\"Accuracy:\", accuracy_score(y_train_bal, y_train_bal))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_train_bal, y_train_bal))\n",
    "\n",
    "# Evaluate the classifier performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2942cbd-17e1-480d-b896-4709f8cf781e",
   "metadata": {},
   "source": [
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00    157425\n",
    "           1       1.00      1.00      1.00    157425\n",
    "\n",
    "    accuracy                           1.00    314850\n",
    "   macro avg       1.00      1.00      1.00    314850\n",
    "weighted avg       1.00      1.00      1.00    314850\n",
    "\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.69      0.86      0.77      7721\n",
    "           1       0.97      0.93      0.95     39393\n",
    "\n",
    "    accuracy                           0.91     47114\n",
    "   macro avg       0.83      0.89      0.86     47114\n",
    "weighted avg       0.93      0.91      0.92     47114\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9dd2fe-1073-496b-ad12-762987f08932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from scipy.stats import randint\n",
    "\n",
    "# # Define the parameter grid/distributions\n",
    "# param_dist = {\n",
    "#     'n_estimators': randint(100, 500),\n",
    "#     'max_depth': [5, 10, 15, 20, 30, 50, None],\n",
    "#     'min_samples_split': [2, 5, 10, 20, 30],\n",
    "#     'min_samples_leaf': [1, 2, 5, 10, 20, ],\n",
    "#     'max_features': ['sqrt', 'log2', 0.5, 0.7, None],\n",
    "#     'bootstrap': [True, False],\n",
    "#     'class_weight': ['balanced']\n",
    "# }\n",
    "\n",
    "# # Instantiate the Random Forest\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # Set up the random search\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=rf,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=50,               # number of combinations to try\n",
    "#     cv=5,                    # 5-fold cross-validation\n",
    "#     scoring='f1_macro',      # or 'accuracy', 'roc_auc', etc.\n",
    "#     n_jobs=-1,               # use all cores\n",
    "#     random_state=42,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# # Fit to training data\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best parameters found\n",
    "# print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "# print(\"Best CV score:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1960b6-fd66-474f-8db7-163d0fc0cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importances = best_model.feature_importances_\n",
    "importances = classifier.feature_importances_\n",
    "\n",
    "# Feature names (must match your input order)\n",
    "feature_names = X_train.columns  \n",
    "\n",
    "# Create a DataFrame for easy plotting\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.barh(feat_imp_df['Feature'], feat_imp_df['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance (cuML)')\n",
    "plt.gca().invert_yaxis()  # Most important at top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e103283-b3c0-4a70-bd2d-b9a51320da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(feat_imp_df['Feature'][0:13])\n",
    "features, len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a1e7e-6544-4407-9986-144497bc6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = subDF[features]\n",
    "y = subDF['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize and train a RandomForest Classifier\n",
    "classifier = RandomForestClassifier(n_estimators=500,\n",
    "                                    max_depth=20,\n",
    "                                    min_samples_leaf= 20,\n",
    "                                    min_samples_split= 10,\n",
    "                                    max_features=0.5,  \n",
    "                                    bootstrap=True,\n",
    "                                    random_state=42,\n",
    "                                    class_weight='balanced',\n",
    "                                    n_jobs=-1)\n",
    "classifier.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier performance\n",
    "print(\"Accuracy:\", accuracy_score(y_train_bal, y_train_bal))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_train_bal, y_train_bal))\n",
    "\n",
    "# Evaluate the classifier performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180ccc90-b881-4b09-80d7-3cd7158325bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(classifier, \"random_forest_goodBadMatchup.joblib\")\n",
    "\n",
    "# # load\n",
    "# loaded_rf = joblib.load(\"my_random_forest.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e04b8-8e58-4dad-bae6-ff3ef1ba8ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.histplot(df.loc[df['label'] == -1], x=xlabel, y = ylabel, bins=1000, \n",
    "            #hue='label', common_norm=True, cbar = True,\n",
    "            palette = sns.color_palette(\"bright\"), ax = ax)\n",
    "ax.set_ylim([-30,30])\n",
    "ax.set_xlim([-3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb81172-622e-4ebe-ad41-f88d4ae2a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Features and Target\n",
    "X = subDF[features].values\n",
    "y = subDF['label'].values\n",
    "\n",
    "# X = df[features].values\n",
    "# y = df['label'].values\n",
    "\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# SMOTE for Class Balancing\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_bal = scaler.fit_transform(X_train_bal)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# Build the Neural Network Model\n",
    "# Best Hyperparameters:\n",
    "# units_input: 96\n",
    "# dropout_input: 0.1\n",
    "# num_layers: 3\n",
    "# units_0: 80\n",
    "# dropout_0: 0.2\n",
    "# lr: 0.0007730555977073046\n",
    "# units_1: 80\n",
    "# dropout_1: 0.0\n",
    "# units_2: 96\n",
    "# dropout_2: 0.30000000000000004\n",
    "model = Sequential([\n",
    "    Dense(96, activation='relu', input_shape=(X_train_bal.shape[1],)),\n",
    "    Dropout(0.1),\n",
    "    Dense(80, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(80, activation='relu'),\n",
    "    Dropout(0.0),\n",
    "    Dense(96, activation='relu'),  \n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early Stopping to Prevent Overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(X_train_bal, y_train_bal, \n",
    "                    validation_split=0.2, \n",
    "                    epochs=100, \n",
    "                    batch_size=64, \n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=1)\n",
    "\n",
    "#plot_training_curves(history)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_prob = model.predict(X_train).flatten()\n",
    "y_pred_train = (y_pred_train_prob > 0.5).astype(int)\n",
    "\n",
    "y_pred_prob = model.predict(X_test).flatten()\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Training Accuracy (Last Epoch):\", history.history['accuracy'][-1])\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_train, y_pred_train))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5625ea-7266-41ea-b047-685b88450561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save('X_train.npy', X_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bde33-ce92-4aeb-a6ff-510761949276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_prob = model.predict(X_test).flatten()\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Training Accuracy (Last Epoch):\", history.history['accuracy'][-1])\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a610c7-cc93-4626-a148-9f4832e8c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_tf_model.h5')     # TensorFlow SavedModel format (folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b48b80-a5f9-41e4-905d-6cf9683285dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "X_new = df[features]\n",
    "y_pred_prob_all = model.predict(scaler.transform(X_new)).flatten()\n",
    "y_pred_all = (y_pred_prob_all > 0.5).astype(int)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Training Accuracy (Last Epoch):\", history.history['accuracy'][-1])\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(df['label']+1, y_pred_all))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(df['label']+1, y_pred_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e86587-80a1-4a8c-bae7-c8cea3365d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame: df must contain 'label', 'speed', 'direction'\n",
    "# Replace these with your actual column names\n",
    "xlabel = 'Speed Difference (QuikSCAT - TAO)'\n",
    "ylabel = 'Direction Difference (QuikSCAT - TAO)'\n",
    "\n",
    "# Compute mean and std\n",
    "mu_speed = df[xlabel].mean()\n",
    "std_speed = df[xlabel].std()\n",
    "mu_dir = df[ylabel].mean()\n",
    "std_dir = df[ylabel].std()\n",
    "\n",
    "### label the data from deep learning\n",
    "df['ML_label'] = y_pred_all\n",
    "\n",
    "# Create overall figure\n",
    "fig = plt.figure(figsize=(13, 10))\n",
    "gs = gridspec.GridSpec(4, 8, width_ratios=[3, 0.8, 0.1, 0.1, 0.25, 0.1, 0.75, 1], height_ratios=[0.75, 3, 0.75, 2], wspace=0, hspace=0)\n",
    "\n",
    "# Joint and marginals (LEFT)\n",
    "ax_joint = fig.add_subplot(gs[1, 0])\n",
    "ax_marg_x = fig.add_subplot(gs[0, 0], sharex=ax_joint)\n",
    "ax_marg_y = fig.add_subplot(gs[1, 1], sharey=ax_joint)\n",
    "ax_cb1 = fig.add_subplot(gs[1,3])\n",
    "ax_cb2 = fig.add_subplot(gs[1,5])\n",
    "\n",
    "# Countplot (RIGHT)\n",
    "ax_count = fig.add_subplot(gs[0:2, -1])  # spans both rows\n",
    "\n",
    "# -------------------------\n",
    "# 1. JOINT HISTOGRAM + KDE\n",
    "\n",
    "colors = [plt.cm.Blues, plt.cm.Oranges]\n",
    "hexcounts = [100, 100]\n",
    "x_min, x_max, y_min, y_max = -5, 5, -100, 100\n",
    "vmins = [0, 0]\n",
    "vmaxs = [100, 500]\n",
    "numcontours = 5\n",
    "\n",
    "for i, (label, ax, ax_cb) in enumerate(zip([0, 1], [ax_joint, ax_joint], [ax_cb1, ax_cb2])):\n",
    "    df_label = df[df['ML_label'] == label]\n",
    "\n",
    "    hb = ax.hexbin(\n",
    "        x=df_label[xlabel],\n",
    "        y=df_label[ylabel],\n",
    "        gridsize=hexcounts[i],\n",
    "        cmap=colors[i],\n",
    "        mincnt=1,\n",
    "        linewidths=0.01,\n",
    "        edgecolors=(0, 0, 0, 0.01),\n",
    "        vmin=vmins[i],            # â† custom lower limit\n",
    "        vmax=vmaxs[i],             # â† custom upper limit\n",
    "        extent=(x_min, x_max, y_min, y_max)\n",
    "    )\n",
    "\n",
    "    # Extract colorbar for this label\n",
    "    cbar = fig.colorbar(hb, cax=ax_cb)\n",
    "    \n",
    "#cbar.set_label(\"Bin Count\")\n",
    "cbar.ax.text(\n",
    "    0.6, 1.05,                        # x=50% of width, y=just above the top\n",
    "    \"Bin Count\",                     # your label\n",
    "    ha='right', va='bottom',        # horizontal center, baseline aligned\n",
    "    fontsize=10, fontweight='bold',  # optional styling\n",
    "    transform=cbar.ax.transAxes      # relative to colorbar axis\n",
    ")\n",
    "\n",
    "# Create the Joint PDF plot and capture the returned QuadContourSet object\n",
    "# KDE Grid resolution\n",
    "nx, ny = 64, 64\n",
    "\n",
    "# Data\n",
    "x = df[xlabel].values\n",
    "y = df[ylabel].values\n",
    "xy = np.vstack([x, y])\n",
    "\n",
    "# KDE with specified bandwidth\n",
    "kde = gaussian_kde(xy, bw_method=0.1)\n",
    "\n",
    "# Grid over which to evaluate\n",
    "xx, yy = np.mgrid[x_min:x_max:nx*1j, y_min:y_max:ny*1j]\n",
    "grid_coords = np.vstack([xx.ravel(), yy.ravel()])\n",
    "z = kde(grid_coords).reshape(xx.shape)\n",
    "\n",
    "# Normalize z to make it a probability surface (area â‰ˆ 1)\n",
    "dx = (x_max - x_min) / nx\n",
    "dy = (y_max - y_min) / ny\n",
    "z_prob = z / z.sum() #/ (dx * dy)\n",
    "\n",
    "# Flatten and sort to get cumulative distribution\n",
    "z_flat = z_prob.flatten()\n",
    "z_sorted = np.sort(z_flat)[::-1]  # ðŸ”„ descending order\n",
    "z_cumsum = np.cumsum(z_sorted)\n",
    "\n",
    "# Desired probability masses for contours\n",
    "mass_levels = [0.1, 0.5, 0.68, 0.8, 0.95]\n",
    "mass_levels.reverse()\n",
    "levels = [z_sorted[np.searchsorted(z_cumsum, mass)] for mass in mass_levels]\n",
    "\n",
    "\n",
    "#levels.sort()\n",
    "# Plot contours\n",
    "contour = ax_joint.contour(xx, yy, z_prob, levels=levels, colors='black', linewidths=0.8)\n",
    "\n",
    "# Label with probability instead of density\n",
    "fmt = {}\n",
    "for l, s in zip(contour.levels, mass_levels):\n",
    "    fmt[l] = f\"{s:.3f}\"  # show enclosed probability\n",
    "\n",
    "#fmt = {level: f\"{mass:.3f}\" for level, mass in zip(levels, mass_levels)}\n",
    "\n",
    "ax_joint.clabel(contour, contour.levels, inline=True, fmt=fmt, fontsize=8)\n",
    "# # Add labels to the contour lines\n",
    "# ax_joint.clabel(contour, inline=True, fontsize=8, fmt=\"%.2f\")\n",
    "\n",
    "# 2. MARGINAL KDEs\n",
    "sns.kdeplot(data=df, x=xlabel, fill=True, legend=False, ax=ax_marg_x, clip = (x_min, x_max) , cut = 0, bw_adjust=0.5 )\n",
    "sns.kdeplot(data=df, y=ylabel, fill=True, legend=False, ax=ax_marg_y, clip = (y_min, y_max) , cut = 0, bw_adjust=0.5 )\n",
    "\n",
    "# Remove marginal axis ticks\n",
    "ax_marg_x.tick_params(bottom=False, labelbottom=False)\n",
    "ax_marg_y.tick_params(left=False, labelleft=False)\n",
    "\n",
    "# Add Â±1Ïƒ lines\n",
    "for x in [mu_speed - std_speed, mu_speed, mu_speed + std_speed]:\n",
    "    ax_joint.axvline(x, color='red', linestyle='--', linewidth=1)\n",
    "    ax_marg_x.axvline(x, color='red', linestyle='--', linewidth=1)  \n",
    "    ax_marg_x.set_yticks([0.2,0.4])\n",
    "\n",
    "for y in [mu_dir - std_dir, mu_dir, mu_dir + std_dir]:\n",
    "    ax_joint.axhline(y, color='green', linestyle='--', linewidth=1)\n",
    "    ax_marg_y.axhline(y, color='green', linestyle='--', linewidth=1)\n",
    "    ax_marg_y.set_xticks([0.02,0.04])\n",
    "\n",
    "\n",
    "# Axis labels\n",
    "ax_joint.set_xlabel(xlabel + ' [m/s]')\n",
    "ax_joint.set_ylabel(ylabel + \" (Â°)\")\n",
    "ax_marg_x.tick_params(axis='x', which='both', bottom=False, labelbottom=False)\n",
    "ax_marg_y.tick_params(axis='y', which='both', left=False, labelleft=False)\n",
    "ax_marg_x.set_xlabel(None)\n",
    "ax_marg_y.set_ylabel(None)\n",
    "\n",
    "# Label the panel\n",
    "ax_joint.text(0.05, 0.95, 'A. (i)', transform=ax_joint.transAxes, fontsize=16, weight='bold')\n",
    "ax_marg_x.text(0.05, 0.80, 'A. (ii)', transform=ax_marg_x.transAxes, fontsize=16, weight='bold')\n",
    "ax_marg_y.text(0.05, 0.95, 'A. (iii)', transform=ax_marg_y.transAxes, fontsize=16, weight='bold')\n",
    "\n",
    "# -------------------------\n",
    "# 3. COUNTPLOT\n",
    "s = sns.countplot(data=df, x='ML_label', palette='bright', ax=ax_count)\n",
    "for container in s.containers:\n",
    "    s.bar_label(container)\n",
    "ax_count.set_title(\"Cluster Label Counts\")\n",
    "#ax_count.set_xticks([-1, 0])                    # positions (the actual cluster labels)\n",
    "ax_count.set_xticklabels([\"Bad\", \"Good\"])       # the new labels shown on the axis\n",
    "ax_count.set_xlabel(\"Matchup Quality\")\n",
    "ax_count.set_ylabel(\"Count\")\n",
    "ax_count.text(0.05, 0.95, 'B.', transform=ax_count.transAxes, fontsize=16, weight='bold')\n",
    "\n",
    "# Final adjustments\n",
    "\n",
    "ax_joint.set_xlim(x_min, x_max)\n",
    "ax_joint.set_ylim(y_min,  y_max)\n",
    "ax_joint.grid(lw = 0.2)\n",
    "\n",
    "ax_marg_x.set_xlim(x_min, x_max)\n",
    "ax_marg_x.grid(lw = 0.2)\n",
    "\n",
    "ax_marg_y.set_ylim(y_min,  y_max)\n",
    "ax_marg_y.grid(lw = 0.2)\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature \n",
    "dotSize = 60\n",
    "slw = 0.1\n",
    "\n",
    "latList = [-9, -8, -5, -2, 0, 2, 5, 8, 9]\n",
    "lonList = [-95, -110, -125, -140, -155, -170, -180, 165]\n",
    "\n",
    "ylen = len(latList)\n",
    "xlen = len(lonList)\n",
    "\n",
    "taskList = []\n",
    "\n",
    "for latId  in range(ylen):\n",
    "    for lonId in range(xlen):\n",
    "        taskList.append([latList[latId], lonList[lonId]])\n",
    "\n",
    "ntasks = len(taskList)\n",
    "\n",
    "ax = fig.add_subplot(gs[3,:], projection=ccrs.PlateCarree(central_longitude=180))\n",
    "ax.set_aspect(1.2)\n",
    "\n",
    "land = cfeature.NaturalEarthFeature(\n",
    "    category='physical',\n",
    "    name='land',\n",
    "    scale='50m',\n",
    "    facecolor='lightgrey'  # Set the color to grey\n",
    ")\n",
    "\n",
    "ax.add_feature(land)\n",
    "\n",
    "\n",
    "ax.set_extent([140, -70, -5, 5]) \n",
    "gridlines = ax.gridlines(draw_labels=True)\n",
    "ax.coastlines()\n",
    "plotList = np.zeros((0,3), dtype=float)\n",
    "for task in taskList:\n",
    "    lat = task[0]\n",
    "    lon = task[1]\n",
    "    txt = getGoodBad(lat, lon, df)\n",
    "    \n",
    "\n",
    "    xpos = lon + 180\n",
    "    \n",
    "    if xpos > 180:\n",
    "       xpos -= 360\n",
    "    if txt != '': \n",
    "        badPercent = getBadPercent(lat, lon, df)\n",
    "        #print(badPercent)\n",
    "        txt = f'{badPercent:2.0f}'\n",
    "        ax.text(xpos+2, lat-0.5, txt)\n",
    "        plotList = np.concatenate((plotList, np.array([[xpos, lat, badPercent]])), axis = 0)\n",
    "\n",
    "x = ax.scatter(plotList[:,0], plotList[:,1], c=plotList[:,2], s=dotSize, edgecolor='black', linewidths=slw, cmap=cm.turbo)    \n",
    "cb = plt.colorbar(x, ax = ax)\n",
    "cb.ax.set_title('% of Bad Matchups')\n",
    "#ax.scatter(df['LONGITUDE'], df['LATITUDE'], transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.text(0.1, 0.95, 'C. ', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = 15)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('clustering.pdf', dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346a281-9c05-457a-886b-159ecd4981cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,  axes = plt.subplots(nrows = 1, ncols = 4, figsize=(12,3), sharex = True, sharey = True)\n",
    "labelFS = 12\n",
    "\n",
    "\n",
    "\n",
    "ax = axes[0]\n",
    "s = sns.histplot( df.loc[df[\"ML_label\"] == 1], x=xlabel, y = ylabel, bins = (100,100), cbar = True, ax = ax)\n",
    "ax.set_xlim(-10,10)\n",
    "ax.set_ylim(-180,180)\n",
    "ax.grid()\n",
    "ax.text(0.1, 0.95, 'A', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = labelFS)\n",
    "\n",
    "ax = axes[1]\n",
    "s = sns.histplot( df.loc[df[\"ML_label\"] == 0], x=xlabel, y = ylabel, bins = (100,100), color = 'orange', cbar = True, ax = ax)\n",
    "ax.set_xlim(-10,10)\n",
    "ax.set_ylim(-180,180)\n",
    "ax.grid()\n",
    "ax.text(0.1, 0.95, 'B', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = labelFS)\n",
    "\n",
    "ax = axes[2]\n",
    "s = sns.histplot( df.loc[df[\"label\"] == 0], x=xlabel, y = ylabel, bins = (20,20), cbar = True, ax = ax)\n",
    "ax.set_xlim(-10,10)\n",
    "ax.set_ylim(-180,180)\n",
    "ax.grid()\n",
    "ax.text(0.1, 0.95, 'C', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = labelFS)\n",
    "\n",
    "ax = axes[3]\n",
    "s = sns.histplot( df.loc[df[\"label\"] == -1], x=xlabel, y = ylabel, bins = (100,100), color = 'orange', cbar = True, ax = ax)\n",
    "ax.set_xlim(-10,10)\n",
    "ax.set_ylim(-180,180)\n",
    "ax.grid()\n",
    "ax.text(0.1, 0.95, 'D', horizontalalignment='left',\n",
    "        verticalalignment='center', transform=ax.transAxes,\n",
    "        weight = 'heavy', fontsize = labelFS)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('goodBadMatchupPredictedOnASCATtrainedOnQuikSCAT.pdf', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060865a-ec89-43e6-8aa2-41b21a2d2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = df[features]\n",
    "y = y_pred_all #subDF['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize and train a RandomForest Classifier\n",
    "# classifier = RandomForestClassifier(n_estimators=300,\n",
    "#                                     max_depth=30,\n",
    "#                                     min_samples_leaf=3,\n",
    "#                                     min_samples_split=5,\n",
    "#                                     bootstrap=True,\n",
    "#                                     random_state=42,\n",
    "#                                     class_weight='balanced',\n",
    "#                                     n_jobs=-1)\n",
    "classifier = RandomForestClassifier(n_estimators=500,\n",
    "                                    max_depth=20,\n",
    "                                    min_samples_leaf= 20,\n",
    "                                    min_samples_split= 10,\n",
    "                                    max_features=0.5,  \n",
    "                                    bootstrap=True,\n",
    "                                    random_state=42,\n",
    "                                    class_weight='balanced',\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "classifier.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier performance\n",
    "print(\"Accuracy:\", accuracy_score(y_train_bal, y_train_bal))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_train_bal, y_train_bal))\n",
    "\n",
    "# Evaluate the classifier performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462678b-f3fe-46c1-bb94-3ed4f90de711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importances = best_model.feature_importances_\n",
    "importances = classifier.feature_importances_\n",
    "\n",
    "# Feature names (must match your input order)\n",
    "feature_names = X_train.columns  \n",
    "\n",
    "# Create a DataFrame for easy plotting\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.barh(feat_imp_df['Feature'], feat_imp_df['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance (cuML)')\n",
    "plt.gca().invert_yaxis()  # Most important at top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e119c-762e-49f6-9f7f-aa0bf8426cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(classifier, \"random_forest_goodBadMatchup_firstNeuralNet.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab75119a-559d-4ebc-8b73-dc2d4783a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras_tuner as kt  # If you're using Keras Tuner v1.x\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# input_dim = X_train_bal.shape[1]\n",
    "\n",
    "# def build_model(hp):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(hp.Int('units_input', 16, 128, step=16), activation='relu', input_shape=(input_dim,)))\n",
    "#     model.add(Dropout(hp.Float('dropout_input', 0.0, 0.5, step=0.1)))\n",
    "\n",
    "#     for i in range(hp.Int('num_layers', 1, 3)):\n",
    "#         model.add(Dense(hp.Int(f'units_{i}', 16, 128, step=16), activation='relu'))\n",
    "#         model.add(Dropout(hp.Float(f'dropout_{i}', 0.0, 0.5, step=0.1)))\n",
    "\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#     model.compile(optimizer=Adam(hp.Float('lr', 1e-4, 1e-2, sampling='LOG')),\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# tuner = kt.RandomSearch(\n",
    "#     build_model,\n",
    "#     objective='val_accuracy',\n",
    "#     max_trials=10,\n",
    "#     executions_per_trial=2,\n",
    "#     directory='tuner_dir',\n",
    "#     project_name='wind_matchup_tuning'\n",
    "# )\n",
    "\n",
    "# tuner.search(X_train_bal, y_train_bal,\n",
    "#              validation_split=0.2,\n",
    "#              epochs=50,\n",
    "#              batch_size=256,\n",
    "#              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "# # Get best model\n",
    "# best_model = tuner.get_best_models(1)[0]\n",
    "# best_model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f4787b-fd04-47db-91a0-026d5131d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "# best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# # Print all selected hyperparameter values\n",
    "# print(\"Best Hyperparameters:\")\n",
    "# for param, value in best_hps.values.items():\n",
    "#     print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d39c3-4c61-4c09-9717-0e8588798b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "\n",
    "# # Get the full summary of trials as a DataFrame\n",
    "# tuner_results = tuner.oracle.get_best_trials(num_trials=10)\n",
    "# results_data = []\n",
    "\n",
    "# for trial in tuner_results:\n",
    "#     trial_summary = trial.metrics.get_last_value('val_accuracy')\n",
    "#     trial_params = trial.hyperparameters.values\n",
    "#     trial_params['val_accuracy'] = trial_summary\n",
    "#     results_data.append(trial_params)\n",
    "\n",
    "# results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# # Plot Validation Accuracy vs. Number of Neurons in Input Layer\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.scatter(results_df['units_input'], results_df['val_accuracy'], c='blue')\n",
    "# plt.xlabel('Neurons in Input Layer')\n",
    "# plt.ylabel('Validation Accuracy')\n",
    "# plt.title('Validation Accuracy vs. Input Layer Size')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot Validation Accuracy vs. Learning Rate\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.scatter(results_df['lr'], results_df['val_accuracy'], c='green')\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('Learning Rate')\n",
    "# plt.ylabel('Validation Accuracy')\n",
    "# plt.title('Validation Accuracy vs. Learning Rate')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # If you want to explore the DataFrame directly\n",
    "# print(results_df.sort_values(by='val_accuracy', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1b63c-8722-48c1-81a3-f08b552a26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predictions\n",
    "# y_pred_prob = best_model.predict(X_test).flatten()\n",
    "# y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# # Evaluation\n",
    "# print(\"Training Accuracy (Last Epoch):\", history.history['accuracy'][-1])\n",
    "# print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c55131-4941-4d49-b7b7-d9f6ab79b08d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
