{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f05aca-9e64-46ad-af1d-4baa9e2fa9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 13:28:12.441314: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-06 13:28:12.471294: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-06 13:28:12.471320: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-06 13:28:12.472693: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-06 13:28:12.477787: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist, cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "import tensorflow as tf\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from matplotlib import cm\n",
    "import matplotlib.cbook as cbook\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset, num2date, date2num\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import cmocean as cmocn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955fc5ae-3535-41ac-be64-a55b2f3cc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('../rainFlagRemovedBuoyDataBadQualityRemovedMatchup.nc')\n",
    "df = ds.to_dataframe()\n",
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ec57e1-b791-4b2f-98e7-0118f4a53ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Speed Difference (QuikSCAT - TAO)'] = df['Wind Speed (QuikSCAT)'] - df['Wind Speed (TAO)']\n",
    "\n",
    "df['Wind Direction (TAO)'] = (-(df['Wind Direction (TAO)'] - 90.0) + 360)%360\n",
    "df['Wind Direction (QuikSCAT)'] = (-(df['Wind Direction (QuikSCAT)'] - 90.0) + 360)%360\n",
    "df['mean WDIR 30min'] = (-(df['mean WDIR 30min'] - 90.0) + 360)%360\n",
    "df['mean WDIR 60min'] = (-(df['mean WDIR 60min'] - 90.0) + 360)%360\n",
    "df['mean WDIR 120min'] = (-(df['mean WDIR 120min'] - 90.0) + 360)%360\n",
    "\n",
    "df['Direction Difference (QuikSCAT - TAO)'] = ((df['Wind Direction (QuikSCAT)'] - df['Wind Direction (TAO)'])+360)%360\n",
    "dirDiff = np.array(df['Direction Difference (QuikSCAT - TAO)'])\n",
    "dirDiff[dirDiff > 180] -= 360\n",
    "df['Direction Difference (QuikSCAT - TAO)'] = dirDiff\n",
    "\n",
    "df['Speed Difference (QuikSCAT - TAO 30 min mean)'] = df['Wind Speed (QuikSCAT)'] - df['mean WSPD 30min']\n",
    "df['Direction Difference (QuikSCAT - TAO 30 min mean)'] = ((df['Wind Direction (QuikSCAT)'] - df['mean WDIR 30min'])+360)%360\n",
    "dirDiff = np.array(df['Direction Difference (QuikSCAT - TAO 30 min mean)'])\n",
    "dirDiff[dirDiff > 180] -= 360\n",
    "df['Direction Difference (QuikSCAT - TAO 30 min mean)'] = dirDiff\n",
    "\n",
    "df['Speed Difference (QuikSCAT - TAO 1 hr mean)'] = df['Wind Speed (QuikSCAT)'] - df['mean WSPD 60min']\n",
    "df['Direction Difference (QuikSCAT - TAO 1 hr mean)'] = ((df['Wind Direction (QuikSCAT)'] - df['mean WDIR 60min'])+360)%360\n",
    "dirDiff = np.array(df['Direction Difference (QuikSCAT - TAO 1 hr mean)'])\n",
    "dirDiff[dirDiff > 180] -= 360\n",
    "df['Direction Difference (QuikSCAT - TAO 1 hr mean)'] = dirDiff\n",
    "\n",
    "df['Speed Difference (QuikSCAT - TAO 2 hr mean)'] = df['Wind Speed (QuikSCAT)'] - df['mean WSPD 120min']\n",
    "df['Direction Difference (QuikSCAT - TAO 2 hr mean)'] = ((df['Wind Direction (QuikSCAT)'] - df['mean WDIR 120min'])+360)%360\n",
    "dirDiff = np.array(df['Direction Difference (QuikSCAT - TAO 2 hr mean)'])\n",
    "dirDiff[dirDiff > 180] -= 360\n",
    "df['Direction Difference (QuikSCAT - TAO 2 hr mean)'] = dirDiff\n",
    "\n",
    "\n",
    "df['Zonal Neutral Wind Speed at 10m (TAO)'] = df['Neutral Wind Speed at 10m (TAO)']*np.cos(np.deg2rad(df['Wind Direction (TAO)']))\n",
    "df['Meridional Neutral Wind Speed at 10m (TAO)'] = df['Neutral Wind Speed at 10m (TAO)']*np.sin(np.deg2rad(df['Wind Direction (TAO)']))\n",
    "\n",
    "df['Zonal Neutral Wind Speed at 10m (QuikSCAT)'] = df['Wind Speed (QuikSCAT)']*np.cos(np.deg2rad(df['Wind Direction (QuikSCAT)']))\n",
    "df['Meridional Neutral Wind Speed at 10m (QuikSCAT)'] = df['Wind Speed (QuikSCAT)']*np.sin(np.deg2rad(df['Wind Direction (QuikSCAT)']))\n",
    "\n",
    "df['Zonal Wind Speed Difference (QuikSCAT - TAO)'] = df['Zonal Neutral Wind Speed at 10m (QuikSCAT)'] - df['Zonal Neutral Wind Speed at 10m (TAO)']\n",
    "df['Meridional Wind Speed Difference (QuikSCAT - TAO)'] = df['Meridional Neutral Wind Speed at 10m (QuikSCAT)'] - df['Meridional Neutral Wind Speed at 10m (TAO)']\n",
    "\n",
    "df['cos(Direction Difference (QuikSCAT - TAO))'] = np.cos(np.deg2rad(df['Direction Difference (QuikSCAT - TAO)']))\n",
    "df['sin(Direction Difference (QuikSCAT - TAO))'] = np.sin(np.deg2rad(df['Direction Difference (QuikSCAT - TAO)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d724b1c1-54be-470e-b8ce-1b391f838237",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = abs(df['Speed Difference (QuikSCAT - TAO)'])\n",
    "x = x - np.mean(x)/np.std(x)\n",
    "\n",
    "y = abs(df['Speed Difference (QuikSCAT - TAO)'])\n",
    "y = y - np.mean(y)/np.std(y)\n",
    "\n",
    "df['distance from origin'] = np.sqrt(x**2 + y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b55e26-f49c-4a0f-88dc-3b2a68c6d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectX = ['Speed Difference (QuikSCAT - TAO)',\n",
    "           'cos(Direction Difference (QuikSCAT - TAO))',\n",
    "           'sin(Direction Difference (QuikSCAT - TAO))']\n",
    "\n",
    "X = df[selectX]\n",
    "normX = (X - X.mean(axis=0))/ X.std(axis=0)\n",
    "#normX = normalize(X.to_numpy())\n",
    "\n",
    "# Function to compute silhouette score for one sample\n",
    "def silhouette_score_sample(i, X, labels, metric='euclidean'):\n",
    "    mask = np.ones(len(X), dtype=bool)\n",
    "    mask[i] = False\n",
    "    current_label = labels[i]\n",
    "    \n",
    "    # Calculate a(i)\n",
    "    a_i = np.mean(pairwise_distances([X[i]], X[labels == current_label], metric=metric)[0])\n",
    "    \n",
    "    # Calculate b(i)\n",
    "    b_i = np.inf\n",
    "    for label in np.unique(labels):\n",
    "        if label == current_label:\n",
    "            continue\n",
    "        b_i = min(b_i, np.mean(pairwise_distances([X[i]], X[labels == label], metric=metric)[0]))\n",
    "    \n",
    "    return (b_i - a_i) / max(a_i, b_i)\n",
    "    \n",
    "def getScore(n_cluster, X):\n",
    "    ## Fit KMeans\n",
    "    kmeans = KMeans(n_clusters=n_cluster, random_state=42, n_init='auto').fit(X)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # # Fit Cosine Similarity\n",
    "    # data = X\n",
    "    # labels, centroids = kmeans_cosine(data, n_cluster)\n",
    "\n",
    "    gmm = GaussianMixture(n_components=4, random_state=0)\n",
    "    gmm.fit(X)\n",
    "    \n",
    "    #Predict the cluster for each data point\n",
    "    labels = gmm.predict(X)\n",
    "    \n",
    "    # Calculate silhouette scores in parallel\n",
    "    n_jobs = -1  # Use all available cores\n",
    "    silhouette_scores = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(silhouette_score_sample)(i, X, labels) for i in range(len(X))\n",
    "    )  \n",
    "    # Calculate the average silhouette score\n",
    "    average_silhouette_score = np.mean(silhouette_scores)\n",
    "\n",
    "    return average_silhouette_score\n",
    "\n",
    "### Function to calculate cosine similarity\n",
    "def kmeans_cosine(X, n_clusters, max_iter=300, tol=1e-4):\n",
    "    # Normalize the data to make it suitable for cosine similarity\n",
    "    X_normalized = normalize(X)\n",
    "\n",
    "    # Randomly initialize the centroids\n",
    "    centroids = X_normalized[np.random.choice(X_normalized.shape[0], n_clusters, replace=False)]\n",
    "    #centroids = X[np.random.choice(X.shape[0], n_clusters, replace=False)]\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Compute cosine distances between points and centroids\n",
    "        distances = cdist(X_normalized, centroids, metric='cosine')\n",
    "        #distances = cdist(X, centroids, metric='cosine')\n",
    "\n",
    "        # Assign clusters based on the closest centroids\n",
    "        clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Calculate new centroids\n",
    "        new_centroids = np.array([X_normalized[clusters == j].mean(axis=0) for j in range(n_clusters)])\n",
    "        #new_centroids = np.array([X[clusters == j].mean(axis=0) for j in range(n_clusters)])\n",
    "\n",
    "        # # Normalize new centroids\n",
    "        new_centroids = normalize(new_centroids)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(centroids, new_centroids, atol=tol):\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return clusters, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "257c4050-34e0-4b1c-969c-4830cfe95d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_709431/3803954138.py:24: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "/tmp/ipykernel_709431/3803954138.py:24: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_cluster \u001b[38;5;129;01min\u001b[39;00m range_n_clusters:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_cluster\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     silhouette_scores\u001b[38;5;241m.\u001b[39mappend(\u001b[43mgetScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_cluster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m silhouette_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(silhouette_scores)\n\u001b[1;32m      8\u001b[0m index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(silhouette_scores)\n",
      "Cell \u001b[0;32mIn[9], line 45\u001b[0m, in \u001b[0;36mgetScore\u001b[0;34m(n_cluster, X)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Calculate silhouette scores in parallel\u001b[39;00m\n\u001b[1;32m     44\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Use all available cores\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m silhouette_scores \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msilhouette_score_sample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Calculate the average silhouette score\u001b[39;00m\n\u001b[1;32m     49\u001b[0m average_silhouette_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(silhouette_scores)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpiNetCDF/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mpiNetCDF/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mpiNetCDF/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "range_n_clusters = np.arange(2,10)\n",
    "silhouette_scores = []\n",
    "for n_cluster in range_n_clusters:\n",
    "    print(f'cluster : {n_cluster}')\n",
    "    silhouette_scores.append(getScore(n_cluster, normX.to_numpy()))\n",
    "\n",
    "silhouette_scores = np.array(silhouette_scores)\n",
    "index = np.argmax(silhouette_scores)\n",
    "best_n_clusters = range_n_clusters[index]\n",
    "best_score = silhouette_scores[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a443d-4b1f-4fca-8a06-933024f0f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range_n_clusters, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(range_n_clusters)\n",
    "plt.grid(True)\n",
    "#plt.show()\n",
    "\n",
    "print(f'The optimal number of clusters is: {best_n_clusters} with a silhouette score of {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79372d-6408-444c-a9aa-fcfda9db5ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
